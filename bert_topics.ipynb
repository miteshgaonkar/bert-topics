{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "bert-topics.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xuM0tnlo0yky",
        "l5CH_Rsfu7Wg"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miteshgaonkar/bert-topics/blob/main/bert_topics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjociS8Nuo4K"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiSpwZ2iXoCR"
      },
      "source": [
        "To make sure your colab doesn't disconnect due to inactivity, you can paste this code in the console of this tab (*right mouse click -> inspect -> Console tab and insert code*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTcAzLYhXc2C"
      },
      "source": [
        "```\n",
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\"); \n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
        "}\n",
        "\n",
        "setInterval(ConnectButton,60000);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuM0tnlo0yky"
      },
      "source": [
        "### Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzqD61nqu_Zj",
        "outputId": "f088e410-0691-4ea4-c7f7-ca73d2003857"
      },
      "source": [
        "# for topic models\n",
        "!pip install -U gensim\n",
        "!pip install pyLDAvis\n",
        "!pip install biterm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (4.3.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (1.2.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (2.8.4)\n",
            "Collecting funcy\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: pandas>=1.3.4 in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (1.5.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (67.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (3.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (1.22.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.3.4->pyLDAvis) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.3.4->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.1.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim->pyLDAvis) (6.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->pyLDAvis) (2.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas>=1.3.4->pyLDAvis) (1.16.0)\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-2.0 pyLDAvis-3.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting biterm\n",
            "  Downloading biterm-0.1.5.tar.gz (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from biterm) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from biterm) (4.65.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from biterm) (0.29.34)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from biterm) (3.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->biterm) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->biterm) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk->biterm) (2022.10.31)\n",
            "Building wheels for collected packages: biterm\n",
            "  Building wheel for biterm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for biterm: filename=biterm-0.1.5-cp39-cp39-linux_x86_64.whl size=271467 sha256=c335890020fd5a024622416244f4d1b058451fc10a53cd418f1299dcc2f89117\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/da/eb/e03a90b2c389f2f4ffba0eb3524d13fa0df43e27db73bf9404\n",
            "Successfully built biterm\n",
            "Installing collected packages: biterm\n",
            "Successfully installed biterm-0.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkKg4D3d03co",
        "outputId": "2aebc7ab-3c45-43e8-a499-62083359e4ed"
      },
      "source": [
        "# !pip install sentence_transformers==0.2.5.1\n",
        "!pip install transformers==2.3.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==2.3.0\n",
            "  Downloading transformers-2.3.0-py3-none-any.whl (447 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.4/447.4 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.26.111-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from transformers==2.3.0) (4.65.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==2.3.0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==2.3.0) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from transformers==2.3.0) (1.22.4)\n",
            "Collecting botocore<1.30.0,>=1.29.111\n",
            "  Downloading botocore-1.29.111-py3-none-any.whl (10.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==2.3.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==2.3.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==2.3.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==2.3.0) (1.26.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==2.3.0) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==2.3.0) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==2.3.0) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.111->boto3->transformers==2.3.0) (2.8.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=90d905f33fd92e413656a08c59126dc3d5d75ad80b00118b2f7f1b8f27ba1d94\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, jmespath, botocore, s3transfer, boto3, transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWeXkRrVPFu0"
      },
      "source": [
        "!python -m textblob.download_corpora"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-ISIntU1Rvr"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4GZRMkfuuO6"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import random\n",
        "from pathlib import Path\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIAFmTYfrhk8"
      },
      "source": [
        "# clustering / topic model modules\n",
        "from sklearn.cluster import KMeans\n",
        "from collections import Counter\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from textblob import TextBlob, Word\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeIkdWPEAu5b"
      },
      "source": [
        "# neural network modules\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "from transformers import (BertTokenizer, BertModel, BertPreTrainedModel, AdamW,\n",
        "                          get_linear_schedule_with_warmup,\n",
        "                          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0Ejaydyt8s-"
      },
      "source": [
        "# lda modules\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "import pyLDAvis\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZXWQ3D9wLsR"
      },
      "source": [
        "# btm modules\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from biterm.utility import vec_to_biterms, topic_summuary\n",
        "from biterm.btm import oBTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Hn45H0nbpR8"
      },
      "source": [
        "# coherence modules\n",
        "\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.coherencemodel import CoherenceModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvuwPZJ96va4"
      },
      "source": [
        "# data downloading\n",
        "import requests\n",
        "from zipfile import ZipFile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsbHxFsZ6uCW"
      },
      "source": [
        "# types\n",
        "from typing import List, Tuple, Dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO922CmwBs0k"
      },
      "source": [
        "# plots\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import ListedColormap\n",
        "from IPython.display import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5CH_Rsfu7Wg"
      },
      "source": [
        "## Download data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIT77KdH3RgI"
      },
      "source": [
        "### Winter Storm Jacob 2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vFSyZVKuvFe"
      },
      "source": [
        "nlwx_df = pd.read_csv('https://drive.google.com/uc?export=download&id=1wYsKTuKabfS7wrVYJ1yu9Sk0cHF7BDhN')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GII84hk0xkTh"
      },
      "source": [
        "#nlwx_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h526UzzW3aoj"
      },
      "source": [
        "### CrisisNLP\n",
        "Dowload CrisisNLP tweets (annotated by paid workers on crowdflower). \n",
        "\n",
        "For more information on dataset see: https://crisisnlp.qcri.org/lrec2016/lrec2016.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKUYFDqb1ZeV"
      },
      "source": [
        "# download already filtered/split datasets\n",
        "CRISIS_NLP_URL = 'https://drive.google.com/uc?export=download&id=1a5eWNUBKYb-RYQ9THU_GLRX2EaBh5spK'\n",
        "DATA_PATH = Path.cwd()\n",
        "CRISIS_NLP_PATH = DATA_PATH / \"LREC_2016_datasets.zip\"\n",
        "\n",
        "r = requests.get(CRISIS_NLP_URL)\n",
        "with CRISIS_NLP_PATH.open(\"wb\") as f:\n",
        "    f.write(r.content)\n",
        "\n",
        "with ZipFile(CRISIS_NLP_PATH) as z:\n",
        "    z.extractall(DATA_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M4cDkEF3eg5"
      },
      "source": [
        "# download and split original data from CrisisNLP (might be rate-limited)\n",
        "DOWNLOAD_ORIGINAL = False \n",
        "\n",
        "if DOWNLOAD_ORIGINAL:\n",
        "    DATA_PATH = Path.cwd()\n",
        "    CRISIS_NLP_PATH = DATA_PATH / \"LREC_2016_datasets.zip\"\n",
        "\n",
        "    CRISIS_NLP_URL = \"https://crisisnlp.qcri.org/data/lrec2016/labeled_cf/CrisisNLP_labeled_data_crowdflower.zip\"\n",
        "    CRISIS_NLP_DIR = \"CrisisNLP_labeled_data_crowdflower\"\n",
        "    label_col = \"choose_one_category\"\n",
        "\n",
        "    r = requests.get(CRISIS_NLP_URL)\n",
        "\n",
        "    with CRISIS_NLP_PATH.open(\"wb\") as f:\n",
        "        f.write(r.content)\n",
        "\n",
        "    with ZipFile(CRISIS_NLP_PATH) as z:\n",
        "        z.extractall(DATA_PATH)\n",
        "\n",
        "    for zip_file in (DATA_PATH / CRISIS_NLP_DIR).glob(\"*/*.zip\"):\n",
        "        print(f\"unzipping {zip_file}\")\n",
        "        with ZipFile(zip_file) as z:\n",
        "            z.extractall(zip_file.parent)\n",
        "\n",
        "\n",
        "    def create_train_test_split(csv_path: Path, label_col, split: float = 0.8):\n",
        "        df = pd.read_csv(csv_path, encoding = \"ISO-8859-1\").sample(frac=1).reset_index(drop=True)\n",
        "        train_df = pd.DataFrame(columns=df.columns)\n",
        "        test_df = pd.DataFrame(columns=df.columns)\n",
        "        train_path = csv_path.parent / f\"{csv_path.stem}_train.csv\"\n",
        "        test_path = csv_path.parent / f\"{csv_path.stem}_test.csv\"\n",
        "        for label in df[label_col].unique():\n",
        "            label_df = df.loc[df[label_col] == label]\n",
        "            train_df = train_df.append(label_df[:int(len(label_df)*split)])\n",
        "            test_df = test_df.append(label_df[int(len(label_df)*split):])\n",
        "\n",
        "        print(f\"Train set:\\n{train_df[label_col].value_counts()}\")\n",
        "        print(f\"Test set:\\n{test_df[label_col].value_counts()}\")\n",
        "        print(\"-\"*80)\n",
        "        train_df.to_csv(train_path)\n",
        "        test_df.to_csv(test_path)\n",
        "\n",
        "\n",
        "    split = 0.8\n",
        "    for csv_path in (DATA_PATH / CRISIS_NLP_DIR).glob(\"*/*.csv\"):\n",
        "        if \"train\" in str(csv_path) or \"test\" in str(csv_path):\n",
        "            continue\n",
        "        else:\n",
        "            print(f\"Splitting: {csv_path.parent}\")\n",
        "            create_train_test_split(csv_path, label_col, split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yQzNq8nvv1D"
      },
      "source": [
        "## FTE Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfrSulS0u-8Q"
      },
      "source": [
        "### Finetune model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8km6oMCQk-O"
      },
      "source": [
        "# skip finetuning and load pre-finetuned model\n",
        "FINETUNE = False\n",
        "\n",
        "# Dataloader\n",
        "BSZ = 8\n",
        "\n",
        "# Optimizer\n",
        "LR = 2e-5\n",
        "EPS = 1e-8 \n",
        "\n",
        "# Training\n",
        "N_EPOCHS = 2 # Model converges within 2 epochs\n",
        "\n",
        "# Random seed\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMXopAYqVqmb"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adRV3DAk21tF"
      },
      "source": [
        "#### Create dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfTkVvefvEg-"
      },
      "source": [
        "if FINETUNE:\n",
        "  # read data\n",
        "  DATA_PATH = Path.cwd() / \"CrisisNLP_labeled_data_crowdflower\"\n",
        "  PATHS = [\"2014_California_Earthquake\", \"2014_Chile_Earthquake_en\", \n",
        "          \"2013_Pakistan_eq\", \"2014_Hurricane_Odile_Mexico_en\",\n",
        "          \"2014_India_floods\", \"2014_Pakistan_floods\", \n",
        "          \"2014_Philippines_Typhoon_Hagupit_en\",\"2015_Cyclone_Pam_en\",\n",
        "          \"2015_Nepal_Earthquake_en\"]\n",
        "\n",
        "  def read_data(\n",
        "          data_dir: Path, \n",
        "          events: List[str], \n",
        "          train_test: str, \n",
        "          text_col: str = \"tweet_text\", \n",
        "          label_col: str = \"choose_one_category\"\n",
        "          ) -> List[Tuple[str, str]]:\n",
        "      data = []\n",
        "      for event in events:\n",
        "          csv_paths = list((data_dir / event).glob(f\"*_{train_test}.csv\"))\n",
        "          assert len(csv_paths) == 1, f\"Found files:{csv_paths}\\n for event {event}\"\n",
        "\n",
        "          df = pd.read_csv(csv_paths[0], encoding = \"ISO-8859-1\")\n",
        "          data.extend(list(zip(df[text_col], df[label_col])))\n",
        "      return(data)\n",
        "\n",
        "  data_train = read_data(DATA_PATH, PATHS, \"train\")\n",
        "  data_val = read_data(DATA_PATH, PATHS, \"test\")\n",
        "\n",
        "  # Note: there seems to be issues with the encoding of the CrisisNLP data\n",
        "  print(f\"Training data: {len(data_train)} examples; e.g. {data_train[0]}\")\n",
        "  print(f\"Validation data: {len(data_val)} examples; e.g. {data_val[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oxSKZLmEJ79"
      },
      "source": [
        "if FINETUNE:\n",
        "  # create data loader\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "  train_labels = [tpl[1] for tpl in data_train]\n",
        "  val_labels = [tpl[1] for tpl in data_val]\n",
        "  ltoi = {l: i for i,l in enumerate(set(train_labels + val_labels))}\n",
        "\n",
        "  def create_dataloader(\n",
        "          data: List[Tuple[str, str]],\n",
        "          tokenizer: BertTokenizer,\n",
        "          ltoi: Dict[str, int],\n",
        "          batch_size: int\n",
        "          ):\n",
        "      texts_raw = [tpl[0] for tpl in data]\n",
        "      d = tokenizer.batch_encode_plus(\n",
        "          texts_raw, add_special_tokens=True, return_tensors='pt',\n",
        "      )\n",
        "      labels = [tpl[1] for tpl in data]\n",
        "      labels = torch.LongTensor([ltoi[l] for l in labels])\n",
        "\n",
        "      ds = TensorDataset(d['input_ids'], d['attention_mask'], labels)\n",
        "      sampler = RandomSampler(ds)\n",
        "      dataloader = DataLoader(ds, sampler=sampler, batch_size=batch_size)\n",
        "      return(dataloader)\n",
        "\n",
        "  train_dataloader = create_dataloader(data_train, tokenizer, ltoi, BSZ)\n",
        "  val_dataloader = create_dataloader(data_val, tokenizer, ltoi, BSZ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynM2H2umGlG-"
      },
      "source": [
        "#### Initialize model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eVTg9nvG15O"
      },
      "source": [
        "class BertForSequenceClassificationOutputPooled(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        outputs = (logits,) + outputs[2:] # add hidden states and attention if they are here\n",
        "        \n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "        \n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47zTEprGPG5o"
      },
      "source": [
        "if FINETUNE:\n",
        "\n",
        "  model = BertForSequenceClassificationOutputPooled.from_pretrained(\n",
        "      \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "      num_labels = len(ltoi), # The number of output labels.\n",
        "      output_attentions = True, # Whether the model returns attentions weights.\n",
        "      output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        "  ).to(device)\n",
        "\n",
        "  optimizer = AdamW(model.parameters(), lr=LR, eps=EPS)\n",
        "  total_steps = len(train_dataloader) * N_EPOCHS\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0,\n",
        "                                              num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfgdAOawRf8d"
      },
      "source": [
        "#### Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSMHFMCrUihY"
      },
      "source": [
        "if FINETUNE:\n",
        "  \n",
        "  def flat_accuracy(preds: np.ndarray, labels: np.ndarray):\n",
        "      pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "      labels_flat = labels.flatten()\n",
        "      return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "      \n",
        "  def format_time(seconds: int):\n",
        "      # Format time as hh:mm:ss\n",
        "      seconds = int(round((seconds))) \n",
        "      return str(datetime.timedelta(seconds=seconds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aLVSlC5P5kv"
      },
      "source": [
        "if FINETUNE:\n",
        "  \n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  loss_values = []\n",
        "  for epoch_i in range(N_EPOCHS):\n",
        "      print('\\n======== Epoch {:} / {:} ========'.format(epoch_i + 1, N_EPOCHS))\n",
        "      print('Training...')\n",
        "      model.train()\n",
        "      total_loss = 0\n",
        "\n",
        "      t0 = time.time()\n",
        "      pbar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
        "      for step, batch in pbar:\n",
        "          b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "          model.zero_grad()        \n",
        "          outputs = model(b_input_ids, \n",
        "                          token_type_ids=None, \n",
        "                          attention_mask=b_input_mask, \n",
        "                          labels=b_labels)\n",
        "\n",
        "          loss = outputs[0]\n",
        "          total_loss += loss.item()\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "          \n",
        "          optimizer.step()\n",
        "          scheduler.step()\n",
        "\n",
        "          logits = outputs[1].detach().cpu().numpy()\n",
        "          label_ids = b_labels.cpu().numpy()\n",
        "          acc = flat_accuracy(logits, label_ids)\n",
        "          pbar.set_description(f\"Acc: {acc}\")\n",
        "\n",
        "      avg_train_loss = total_loss / len(train_dataloader)            \n",
        "      loss_values.append(avg_train_loss)\n",
        "      print(\"\\n  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "      print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "\n",
        "      print(\"\\nRunning Validation...\")\n",
        "      model.eval()\n",
        "      eval_loss, eval_accuracy = 0, 0\n",
        "      nb_eval_steps, nb_eval_examples = 0, 0\n",
        "      \n",
        "      t0 = time.time()\n",
        "      for batch in val_dataloader:\n",
        "          b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "          with torch.no_grad():  \n",
        "              outputs = model(b_input_ids, \n",
        "                              token_type_ids=None, \n",
        "                              attention_mask=b_input_mask)\n",
        "\n",
        "          logits = outputs[0]\n",
        "\n",
        "          logits = logits.cpu().numpy()\n",
        "          label_ids = b_labels.cpu().numpy()\n",
        "          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "          eval_accuracy += tmp_eval_accuracy\n",
        "          nb_eval_steps += 1\n",
        "\n",
        "      print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "      print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Training complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHHYNRUQ_W8L"
      },
      "source": [
        "#### Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_up7EFs-4u6"
      },
      "source": [
        "if FINETUNE:\n",
        "  \n",
        "  OUT_PATH = Path.cwd() / \"model\"\n",
        "  OUT_PATH.mkdir(parents=True)\n",
        "  model.save_pretrained(OUT_PATH)\n",
        "  tokenizer.save_pretrained(OUT_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mxroo66fveas"
      },
      "source": [
        "### Embed tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLzhM-AgG4ae"
      },
      "source": [
        "#### Initialize model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L82yqp-FKJ-M"
      },
      "source": [
        "class BertForClustering(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "    ):\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "        output, pooled, hidden, attention = outputs\n",
        "\n",
        "        pad_mask = (input_ids != self.config.pad_token_id).unsqueeze(-1)\n",
        "        mean_pooled = (output * pad_mask).sum(axis=1) / pad_mask.sum(axis=1)\n",
        "        \n",
        "        # attention shape: (layer, bsz, head, token, attention)\n",
        "        # want attention of CLS token (in position 0) from last layer\n",
        "        # averaged over heads and normalized\n",
        "        cls_attn = attention[-1][:,:,0,:].mean(axis=1)\n",
        "        cls_attn = cls_attn / cls_attn.sum(axis=1).unsqueeze(-1)\n",
        "\n",
        "        return mean_pooled, cls_attn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSK-idqDvfjR"
      },
      "source": [
        "# helper method to get word-level attentions from token-level\n",
        "def get_word_level_attns(\n",
        "        input_ids: torch.LongTensor, \n",
        "        cls_attn: torch.FloatTensor, \n",
        "        ids_to_tokens: Dict[int, str],\n",
        "        ) -> Tuple[List[str], List[float]]:\n",
        "    batch_words = []\n",
        "    batch_attn_scores = []\n",
        "    num_subwords = 1\n",
        "    for i, (iids, attns) in enumerate(zip(input_ids, cls_attn)):\n",
        "        batch_words += [[]]\n",
        "        batch_attn_scores += [[]]\n",
        "        for iid, attn in zip(iids, attns):\n",
        "            token = ids_to_tokens[iid.item()]\n",
        "            if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
        "                continue\n",
        "            if token.startswith(\"##\"):\n",
        "                batch_words[i][-1] += token[2:]\n",
        "                batch_attn_scores[i][-1] += attn.item()\n",
        "                num_subwords += 1\n",
        "            else:\n",
        "                # average score of previous word (if it exists)\n",
        "                if len(batch_attn_scores[i]):\n",
        "                    batch_attn_scores[i][-1] /= num_subwords\n",
        "                    num_subwords = 1\n",
        "                # start new word\n",
        "                batch_words[i] += [token]\n",
        "                batch_attn_scores[i] += [attn.item()]\n",
        "    return batch_words, batch_attn_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khQX10OOXSlU"
      },
      "source": [
        "if FINETUNE:\n",
        "  MODEL_PATH = Path.cwd() / \"model\"\n",
        "else:\n",
        "  if not Path(\"model_save_attention_1epoch\").exists():\n",
        "    !gdown --id 1jnQVZpdsFIAbV8J7szUjtfQ0uXo_NOh_\n",
        "    !unzip model_save_attention_1epoch.zip\n",
        "  MODEL_PATH = Path.cwd() / \"model_save_attention_1epoch\"\n",
        "\n",
        "\n",
        "model = BertForClustering.from_pretrained(MODEL_PATH).to(device).eval()\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH, do_lower_case=True)\n",
        "model.config.pad_token_id = tokenizer.pad_token_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibunwOnUsNyM"
      },
      "source": [
        "bert_baseline = BertForClustering.from_pretrained(\n",
        "    'bert-base-uncased',output_attentions=True, output_hidden_states=True\n",
        "    ).to(device).eval()\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "bert_baseline.config.pad_token_id = tokenizer.pad_token_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv7JGACYgy3I"
      },
      "source": [
        "#### Compute embeds and attentions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytgPMicPhO1o"
      },
      "source": [
        "input_ids = tokenizer.batch_encode_plus(nlwx_df.text, add_special_tokens=True, \n",
        "                                        return_tensors='pt')['input_ids']\n",
        "print(f\"Encoded {len(input_ids)} examples, with max length {input_ids.shape[-1]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8Jox2HGiXMo"
      },
      "source": [
        "EMBED_BSZ = 128\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "batch_embeds = []\n",
        "batch_words = []\n",
        "batch_word_attns = []\n",
        "batch_embeds_bert = []\n",
        "batch_words_bert = []\n",
        "batch_word_attns_bert = []\n",
        "for i in tqdm(list(range(0, len(input_ids), EMBED_BSZ))):\n",
        "    batch = input_ids[i:i+EMBED_BSZ].to(device)\n",
        "    with torch.no_grad():\n",
        "        embeds, cls_attn = model(batch)\n",
        "        words, attns = get_word_level_attns(batch, cls_attn, tokenizer.ids_to_tokens)\n",
        "        embeds_bert, cls_attn_bert = bert_baseline(batch)\n",
        "        words_bert, attns_bert = get_word_level_attns(batch, cls_attn_bert, tokenizer.ids_to_tokens)\n",
        "    batch_embeds += [embeds.cpu().numpy()]\n",
        "    batch_words += [words]\n",
        "    batch_word_attns += [attns]\n",
        "    batch_embeds_bert += [embeds_bert.cpu().numpy()]\n",
        "    batch_words_bert += [words_bert]\n",
        "    batch_word_attns_bert += [attns_bert]\n",
        "\n",
        "# concatenate batches\n",
        "batch_embeds = np.concatenate(batch_embeds)\n",
        "batch_words = list(itertools.chain(*batch_words))\n",
        "batch_word_attns = list(itertools.chain(*batch_word_attns))\n",
        "batch_embeds_bert = np.concatenate(batch_embeds_bert)\n",
        "batch_words_bert = list(itertools.chain(*batch_words_bert))\n",
        "batch_word_attns_bert = list(itertools.chain(*batch_word_attns_bert))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWcWqf83vgnq"
      },
      "source": [
        "### Cluster tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fWzCICAVb02"
      },
      "source": [
        "def topics_df(topics, components, n_words = 20):\n",
        "    df = {}\n",
        "    for i in range(topics):\n",
        "        words = sorted(components[i], key=components[i].get, reverse=True)[:n_words]\n",
        "        df['topic %d' % (i)] = words\n",
        "        if len(words) < n_words:\n",
        "            df['topic %d' % (i)].extend([''] * (n_words - len(words)))\n",
        "    return pd.DataFrame.from_dict(df)\n",
        "\n",
        "def get_stopwords(hashtags = [], \n",
        "                  URL = 'https://drive.google.com/uc?export=download&id=1DcYzpMB-3Dbp7IEtmTbTd4DIgtz9ut6I'):\n",
        "  r = requests.get(URL)\n",
        "  stopwords = json.loads(r.text)\n",
        "  stopwords.extend(['#', '@', '…', \"'\", \"’\", \"[unk]\", \"\\\"\", \";\", \n",
        "                    \"*\", \"_\", \"amp\", \"&\", \"“\", \"”\"] + hashtags)\n",
        "  return(stopwords)\n",
        "\n",
        "stopwords_bert = get_stopwords()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-DPQKQJUjLy"
      },
      "source": [
        "URL_RE = r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
        "\n",
        "class BertTopicModel():\n",
        "    def __init__(self, texts, attentions, embeddings, n_clusters):\n",
        "        self.texts = texts\n",
        "        self.attentions = attentions\n",
        "        self.embeddings = embeddings\n",
        "        self.n_clusters = n_clusters\n",
        "        self.filtered_a = []\n",
        "        self.filtered_t = [] \n",
        "        self.filtered_l = []\n",
        "        \n",
        "    def get_clusters(self):\n",
        "        print(\"Fitting kmeans model.\")\n",
        "        kmeans = KMeans(n_clusters = self.n_clusters, random_state = 0).fit(self.embeddings)\n",
        "        self.labels = kmeans.labels_\n",
        "        self._get_label_counts()\n",
        "        return self.labels\n",
        "    \n",
        "    def get_features(self, stopwords, phrasing = False,min_count=5, threshold=100):\n",
        "        self._filter_data(stopwords)\n",
        "        if phrasing == True:\n",
        "            self._get_phrases(min_count=min_count, threshold=threshold)\n",
        "        else:\n",
        "            self.features = self.filtered_t\n",
        "        print(len(self.features))\n",
        "        self._remove_ifreq_words()\n",
        "        print(len(self.features))\n",
        "        return self.features\n",
        "        \n",
        "    def _filter_data(self, stopwords):\n",
        "        url_re = URL_RE\n",
        "        print(\"Filtering attentions.\")\n",
        "        for idx, a in enumerate(self.attentions):\n",
        "            f = [(i[0].lower(), i[1]) for i in a]\n",
        "            f = [(Word(i[0]).lemmatize(), i[1]) \n",
        "                 for i in f if (i[0] not in stopwords) \n",
        "                 and (not re.match(url_re, i[0]))\n",
        "                 and (i[0].find('snowmageddon2020') == -1)]\n",
        "            f_txt = [re.sub('[^a-zA-Z]+', '', w[0]) for w in f]\n",
        "            f = [(re.sub('[^a-zA-Z]+', '', w[0]), w[1]) for w in f]\n",
        "            if len(f) > 1:\n",
        "                self.filtered_a.append(f)\n",
        "                self.filtered_t.append(f_txt)\n",
        "                self.filtered_l.append(self.labels[idx])\n",
        "                \n",
        "    def determine_cluster_components(self, ngram):\n",
        "        print(\"\"\"\n",
        "    Determining cluster components. This will take a while. \n",
        "    Progress will be printed for every 500th processed property.\n",
        "        \"\"\")\n",
        "        components = {}\n",
        "        words_label = {}\n",
        "        start_time = time.time()\n",
        "        for idx, label in enumerate(self.filtered_l):\n",
        "            if label not in components:\n",
        "                components[label] = {}\n",
        "                words_label[label] = []\n",
        "            else:\n",
        "                f = self._generate_ngram(self.filtered_a[idx], ngram)\n",
        "                for w in f:\n",
        "                    word = ' '.join([r[0] for r in w])\n",
        "                    score = np.mean([r[1] for r in w])\n",
        "                    if word in self.features[idx]:\n",
        "                        if word in components[label]:\n",
        "                            components[label][word] += score\n",
        "                        else:\n",
        "                            components[label][word] = score\n",
        "                        words_label[label].append(word)\n",
        "            if (idx + 1) % 5000 == 0:\n",
        "                print(f'Processed {(idx + 1)} texts in {round(time.time() - start_time, 2)} seconds.')\n",
        "\n",
        "        print(f\"Finished determining a total of {idx + 1} cluster components.\\\n",
        "        Total time {round(time.time() - start_time, 2)} seconds.\")\n",
        "        self.components = components \n",
        "        self.words_label = words_label\n",
        "        return self.components, self.words_label\n",
        "    \n",
        "    def get_tfidf_components(self, max_df = 1.0, stf = False):\n",
        "        tfidf_indexed = self._tf_icf(max_df, stf)\n",
        "        components_tfidf_attn = {}\n",
        "        components_tfidf = {}\n",
        "        for k1 in self.components:\n",
        "            components_tfidf_attn[k1] = {}\n",
        "            components_tfidf[k1] = {}\n",
        "            for k2 in self.components[k1]:\n",
        "                try:\n",
        "                    components_tfidf_attn[k1][k2] = tfidf_indexed[k1][k2] * self.components[k1][k2]\n",
        "                    components_tfidf[k1][k2] = tfidf_indexed[k1][k2]\n",
        "                except:\n",
        "                    continue\n",
        "        self.components_tfidf = components_tfidf\n",
        "        self.components_tfidf_attn = components_tfidf_attn\n",
        "        return self.components_tfidf,self.components_tfidf_attn\n",
        "    \n",
        "    def reset_model(self):\n",
        "        self.filtered_a = []\n",
        "        self.filtered_t = [] \n",
        "        self.filtered_l = []\n",
        "        self.components = [] \n",
        "        self.words_label = []\n",
        "        self.features = []\n",
        "        self.words_label = []\n",
        "        self.components_tfidf = []\n",
        "        self.components_tfidf_attn = []\n",
        "    \n",
        "    def _get_label_counts(self):\n",
        "        unique, counts = np.unique(self.labels, return_counts=True)\n",
        "        print(\"The number of texts per label are:\")\n",
        "        print(dict(zip(unique, counts)))\n",
        "        \n",
        "    # Generate phrases for topic cluster components using Gensim Phrases() and Phraser() functions:\n",
        "    # https://radimrehurek.com/gensim/models/phrases.html\n",
        "    def _get_phrases(self, min_count=5, threshold=100):\n",
        "        bigram = Phrases(self.filtered_t, min_count=min_count, threshold = threshold) # higher threshold fewer phrases.\n",
        "        trigram = Phrases(bigram[self.filtered_t])  \n",
        "\n",
        "        # 'Phraser' is a wrapper that makes 'Phrases' run faster\n",
        "        bigram_phraser = Phraser(bigram)\n",
        "        trigram_phraser = Phraser(trigram)\n",
        "\n",
        "        phrased_bi = [b for b in bigram[self.filtered_t]]\n",
        "        phrased_tri = [t for t in trigram[[b for b in bigram[self.filtered_t]]]]\n",
        "        self.features = [[w.replace('_', ' ') for w in sublist] for sublist in phrased_bi]\n",
        "        \n",
        "    def _remove_ifreq_words(self, vocab_threshold = 10):\n",
        "        texts = [word for words in self.features for word in words]\n",
        "        vocab = self._get_frequent_vocab(texts, threshold = vocab_threshold)\n",
        "        updated_f = []\n",
        "        for f in self.features:\n",
        "            updated_f.append([word for word in f if word in vocab and len(word) > 0])\n",
        "        self.features = updated_f\n",
        "    \n",
        "    def _get_frequent_vocab(self, corpus, threshold=10):\n",
        "        '''\n",
        "        Gets words whose frequency exceeds that of the threshold\n",
        "        in a given corpus.\n",
        "        :param corpus: list of tokenized words\n",
        "        :param threshold:\n",
        "        :return: list of words with higher frequency than threshold\n",
        "        '''\n",
        "        freq = Counter(corpus)\n",
        "        filtered = [word for word, count in freq.items() if count >= threshold]\n",
        "        return filtered\n",
        "    \n",
        "    def _generate_ngram(self, \n",
        "                        seq, \n",
        "                        ngram = (1, 3)):\n",
        "        g = []\n",
        "        for i in range(ngram[0], ngram[-1] + 1):\n",
        "            g.extend(list(self._ngrams_generator(seq, i)))\n",
        "        return g\n",
        "    \n",
        "    def _ngrams_generator(\n",
        "        self,\n",
        "        sequence,\n",
        "        n,\n",
        "        pad_left = False,\n",
        "        pad_right = False,\n",
        "        left_pad_symbol = None,\n",
        "        right_pad_symbol = None):\n",
        "        \"\"\"\n",
        "        generate ngrams.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        sequence : list of str\n",
        "            list of tokenize words.\n",
        "        n : int\n",
        "            ngram size\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ngram: list\n",
        "        \"\"\"\n",
        "        sequence = self._pad_sequence(\n",
        "            sequence, n, pad_left, pad_right, left_pad_symbol, right_pad_symbol\n",
        "        )\n",
        "\n",
        "        history = []\n",
        "        while n > 1:\n",
        "            try:\n",
        "                next_item = next(sequence)\n",
        "            except StopIteration:\n",
        "                return\n",
        "            history.append(next_item)\n",
        "            n -= 1\n",
        "        for item in sequence:\n",
        "            history.append(item)\n",
        "            yield tuple(history)\n",
        "            del history[0]\n",
        "            \n",
        "    # Pad sequence helper funtion for ngram generator.\n",
        "    # Implemented from block 14 of commit 9895ee0 at:\n",
        "    # https://github.com/huseinzol05/NLP-Models-Tensorflow/blob/master/topic-model/2.bert-topic.ipynb\n",
        "    def _pad_sequence(\n",
        "        self,\n",
        "        sequence,\n",
        "        n,\n",
        "        pad_left = False,\n",
        "        pad_right = False,\n",
        "        left_pad_symbol = None,\n",
        "        right_pad_symbol = None):\n",
        "        \n",
        "        sequence = iter(sequence)\n",
        "        if pad_left:\n",
        "            sequence = itertools.chain((left_pad_symbol,) * (n - 1), sequence)\n",
        "        if pad_right:\n",
        "            sequence = itertools.chain(sequence, (right_pad_symbol,) * (n - 1))\n",
        "        return sequence\n",
        "    \n",
        "    def _tf_icf(self, max_df, stf):\n",
        "        def dummy_fun(doc):\n",
        "            return doc\n",
        "        \n",
        "        tfidf_vectorizer = TfidfVectorizer(\n",
        "            analyzer='word',\n",
        "            tokenizer=dummy_fun,\n",
        "            preprocessor=dummy_fun,\n",
        "            token_pattern=None,\n",
        "            max_df = max_df,\n",
        "            sublinear_tf = stf)\n",
        "\n",
        "        tf_idf_corpus = [[item for item in self.words_label[key]] for key in range(self.n_clusters)]\n",
        "        transformed = tfidf_vectorizer.fit_transform(tf_idf_corpus)\n",
        "        index_value={i[1]:i[0] for i in tfidf_vectorizer.vocabulary_.items()}\n",
        "        fully_indexed = []\n",
        "        for row in transformed:\n",
        "            fully_indexed.append({index_value[column]:value for (column,value) in zip(row.indices,row.data)})\n",
        "        return(fully_indexed)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnolHVVGRR9b"
      },
      "source": [
        "class TopicSearch():\n",
        "    def __init__(self,\n",
        "                 embeddings = batch_embeds,\n",
        "                 texts = batch_words,\n",
        "                 attns = batch_word_attns,\n",
        "                 stopwords=stopwords_bert):\n",
        "        self.topics = {}\n",
        "        self.embeddings = embeddings\n",
        "        self.texts = texts\n",
        "        self.attns = attns\n",
        "        self.hyperparams = {}\n",
        "        self.stopwords = stopwords\n",
        "\n",
        "    def search(self,\n",
        "               ngrams=[1,2], \n",
        "               max_df=[0.6, 0.7, 0.8, 0.85, 0.9, 1.0],\n",
        "               stf=[True, False],\n",
        "               n_topics=[5,10,15],\n",
        "               hashtags=[],\n",
        "               hashtag_opts=[True, False],\n",
        "               phrasing=[True, False],\n",
        "               p_min_count=5,\n",
        "               p_threshold=100,\n",
        "               embedding_name='fte'):\n",
        "\n",
        "      e = embedding_name\n",
        "      self.hyperparams[e] = {}\n",
        "      for t in n_topics:\n",
        "        self._init_model(t)\n",
        "        self.hyperparams[e][t] = {}\n",
        "        self.hyperparams[e][t]['hashtags'] = {}\n",
        "        for h in hashtag_opts:\n",
        "          self.hyperparams[e][t]['hashtags'][h] = {}\n",
        "          self.hyperparams[e][t]['hashtags'][h]['ngram'] = {}\n",
        "          for n in ngrams:\n",
        "            ngram = (1, n)\n",
        "            self.hyperparams[e][t]['hashtags'][h]['ngram'][n] = {}\n",
        "            self.hyperparams[e][t]['hashtags'][h]['ngram'][n]['stf'] = {}\n",
        "            for s in stf:\n",
        "              self.hyperparams[e][t]['hashtags'][h]['ngram'][n]['stf'][s] = {}\n",
        "              self.hyperparams[e][t]['hashtags'][h]['ngram'][n]['stf'][s]['max_df'] = {}\n",
        "              for m in max_df:\n",
        "                self.hyperparams[e][t]['hashtags'][h]['ngram'][n]['stf'][s]['max_df'][m] = {}\n",
        "                self.hyperparams[e][t]['hashtags'][h]['ngram'][n]['stf'][s]['max_df'][m]['phrasing'] = {}\n",
        "                for p in phrasing:\n",
        "                  self.hyperparams[e][t]['hashtags'][h]['ngram'][n]['stf'][s]['max_df'][m]['phrasing'][p] = {}\n",
        "                  features = self.bertTM.get_features(stopwords=self.stopwords,\n",
        "                                                      phrasing=p,\n",
        "                                                      min_count=p_min_count,\n",
        "                                                      threshold=p_threshold)\n",
        "                  print(f'Getting components for kmeans model for '\n",
        "                        f'{t} topics, phrasing {p}, {n} ngrams, mdf {m} and stf {s}.')\n",
        "                  components, words_label = self.bertTM.determine_cluster_components(ngram)\n",
        "                  components_tfidf, components_tfidf_attn = self.bertTM.get_tfidf_components(max_df = m,\n",
        "                                                                                            stf = s)\n",
        "                  print(f'Determining topics for kmeans model with '\n",
        "                        f'{t} topics, phrasing {p}, {n} ngrams, mdf {m} and stf {s}.')\n",
        "                  self._store_hyperparams(e,t,h,s,m,p, n, features, \n",
        "                                          components, components_tfidf, components_tfidf_attn)\n",
        "                  self.bertTM.reset_model()\n",
        "\n",
        "    def _store_hyperparams(self,e,t,h,s,m,p, n, features, \n",
        "                           c, ctf, ctfa):\n",
        "        self.hyperparams[e][t]['hashtags'][h]['ngram'][n]['stf'][s]['max_df'][m]['phrasing'][p]['features'] = features\n",
        "        self.hyperparams[e][t]['hashtags'][h]['ngram'][n]['stf'][s]['max_df'][m]['phrasing'][p]['topics_attn'] = self._gen_topics(t, c)\n",
        "        self.hyperparams[e][t]['hashtags'][h]['ngram'][n]['stf'][s]['max_df'][m]['phrasing'][p]['topics_tfidf'] = self._gen_topics(t, ctf)\n",
        "        self.hyperparams[e][t]['hashtags'][h]['ngram'][n]['stf'][s]['max_df'][m]['phrasing'][p]['topics_tfidf_attn'] = self._gen_topics(t, ctfa)\n",
        "    \n",
        "    def _cluster_embeds(\n",
        "        n_clusters: int\n",
        "        ) -> Tuple[KMeans, np.ndarray]:\n",
        "        kmeans = KMeans(n_clusters = N_CLUSTERS, random_state = 0)\n",
        "        self.klabels = kmeans.fit_predict(self.embeddings)\n",
        "        unique, counts = np.unique(kmeans.labels_, return_counts=True)\n",
        "        print(\"The number of texts per label are:\")\n",
        "        print(dict(zip(unique, counts)))\n",
        "\n",
        "    def _init_model(self, t):\n",
        "        # Run kmeans model\n",
        "        print(f\"Running kmeans model with {t} topics.\")\n",
        "        self.bertTM = BertTopicModel(\n",
        "            texts = self.texts, \n",
        "            attentions = self.attns, \n",
        "            embeddings = self.embeddings,\n",
        "            n_clusters = t)\n",
        "        self.labels = self.bertTM.get_clusters()\n",
        "    \n",
        "    def _gen_topics(self, t, components, n_words = 10):\n",
        "        print(f\"Determining topics for kmeans model for with {t} topics.\")\n",
        "        topics = topics_df(topics = t, components = components, n_words = n_words)\n",
        "        return topics\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GCLZJi_UP9e"
      },
      "source": [
        "ngrams=[1]\n",
        "max_df=[0.7, 0.8, 0.85]\n",
        "stf=[True]\n",
        "n_topics=[5,9,10,15]\n",
        "hashtag_opts=[False]\n",
        "phrasing=[False]\n",
        "p_min_count=5\n",
        "p_threshold=100\n",
        "hashtags =  ['nlwhiteout', 'nlweather', 'newfoundland', \n",
        "             'nlblizzard2020', 'nlstm2020','snowmaggedon2020', \n",
        "             'stmageddon2020','stormageddon2020', 'newfoundland2020',\n",
        "             'snowpocalypse2020','snowmageddon','nlstm', \n",
        "             'nlwx', 'nlwx2020','nlblizzard', 'nlstorm2020']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EJwoea_RX8j"
      },
      "source": [
        "batch_words_attn_pairs = [list(zip(batch_words[i], batch_word_attns[i])) for i in range(0, len(batch_words))] \n",
        "\n",
        "ts = TopicSearch(embeddings = batch_embeds,\n",
        "                 texts = batch_words,\n",
        "                 attns = batch_words_attn_pairs,\n",
        "                 stopwords=stopwords_bert)\n",
        "ts.search(ngrams=ngrams,\n",
        "          max_df=max_df,\n",
        "          stf=stf,\n",
        "          n_topics = n_topics,\n",
        "          hashtags = hashtags,\n",
        "          hashtag_opts = hashtag_opts,\n",
        "          phrasing = phrasing,\n",
        "          p_min_count=5,\n",
        "          p_threshold=100,\n",
        "          embedding_name='fte')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcsj9YnZv8DZ"
      },
      "source": [
        "## Baselines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6E1UD7Qv6c3"
      },
      "source": [
        "batch_words_attn_pairs_bert = [list(zip(batch_words_bert[i], batch_word_attns_bert[i])) for i in range(0, len(batch_words_bert))] \n",
        "\n",
        "ts_bert = TopicSearch(embeddings = batch_embeds_bert,\n",
        "                 texts = batch_words_bert,\n",
        "                 attns = batch_words_attn_pairs_bert,\n",
        "                 stopwords=stopwords_bert)\n",
        "ts_bert.search(ngrams=ngrams,\n",
        "          max_df=max_df,\n",
        "          stf=stf,\n",
        "          n_topics = n_topics,\n",
        "          hashtags = hashtags,\n",
        "          hashtag_opts = hashtag_opts,\n",
        "          phrasing = phrasing,\n",
        "          p_min_count=5,\n",
        "          p_threshold=100,\n",
        "          embedding_name='bert')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylRRAUz6Yn4u"
      },
      "source": [
        "trainLDA = False\n",
        "trainBTM = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSGszm18dFZ0"
      },
      "source": [
        "if not (trainLDA or trainBTM):\n",
        "  if not Path(\"lda_coherence\").exists():\n",
        "    !gdown --id 1WaB0v5qlBIhp0RIORO_fNiWNcoSwiC6w\n",
        "\n",
        "  lda_df = pd.read_csv(\"lda_coherence.csv\")\n",
        "\n",
        "if not trainLDA:\n",
        "  if not Path(\"lda_labels\").exists():\n",
        "    !gdown --id 14lk_2gTzPoaO6CfXcvSc93tWpOcJ-FE-\n",
        "\n",
        "  lda_labels = pd.read_csv(\"lda_labels.csv\")\n",
        "\n",
        "if not trainBTM:\n",
        "  if not Path(\"btm_labels\").exists():\n",
        "    !gdown --id 1gnudbXBS0mK_tq2ayxkn94XM31xrIqbT\n",
        "\n",
        "  btm_labels = pd.read_csv(\"btm_labels.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmMoHZiBzvvD"
      },
      "source": [
        "### Preprocessing and Util"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i63UCRPyXtx"
      },
      "source": [
        "if (trainLDA or trainBTM):\n",
        "\n",
        "  def get_frequent_vocab(corpus, threshold=0):\n",
        "      '''\n",
        "      Gets words whose frequency exceeds that of the threshold\n",
        "      in a given corpus.\n",
        "\n",
        "      :param corpus: list of tokenized words\n",
        "      :param threshold:\n",
        "      :return: list of words with higher frequency than threshold\n",
        "      '''\n",
        "      vect = CountVectorizer().fit(corpus)\n",
        "      bag_of_words = vect.transform(corpus)\n",
        "      sum_words = bag_of_words.sum(axis=0)\n",
        "      freq = {word: sum_words[0, idx] for word, idx in vect.vocabulary_.items()}\n",
        "      return [word for word, count in freq.items() if count >= threshold]\n",
        "\n",
        "  def preprocess(df,\n",
        "                  extra_stopwords,\n",
        "                  tweet_col='text',\n",
        "                  to_lower = True,\n",
        "                  include_bigram = False,\n",
        "                  vocab_threshold = 10\n",
        "                  ):\n",
        "      '''\n",
        "      Preprocesses text in a certain dataframe column.\n",
        "\n",
        "      :param df: dataframe to be processed\n",
        "      :param tweet_col: name of dataframe column containing text\n",
        "      :param to_lower: flag whether to convert to lowercase\n",
        "      :param include_bigram: flag whether to include bigrams\n",
        "      :param vocab_threshold: minimum number of words in each tweet\n",
        "      :param extra_stopwords: extra stopwords to add onto default nltk\n",
        "      :return: original df with extra column, 'tokenized_[tweet_col]'\n",
        "      '''\n",
        "      df_copy = df.copy()\n",
        "\n",
        "      # drop rows with empty values\n",
        "      df_copy.dropna(how='all', inplace=True)\n",
        "      # drop rows with identical text\n",
        "      df_copy.drop_duplicates(subset = 'text', inplace = True)\n",
        "\n",
        "      # lower the tweets\n",
        "      if to_lower:\n",
        "          df_copy['preprocessed_' + tweet_col] = df_copy[tweet_col].str.lower()\n",
        "      else:\n",
        "          df_copy['preprocessed_' + tweet_col] = df_copy[tweet_col].str\n",
        "\n",
        "      # filter out stop words and URLs\n",
        "      en_stop_words = set(stopwords.words('english'))\n",
        "\n",
        "      #extended stop words for twitter and extra stop words from user\n",
        "      extended_stop_words = en_stop_words.union({\n",
        "                                '&amp;', 'rt',\n",
        "                                'th', 'co', 're', 've', 'kim', 'daca'\n",
        "                            }).union(extra_stopwords)\n",
        "\n",
        "      #removes urls\n",
        "      url_re = r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
        "      df_copy['preprocessed_' + tweet_col] = df_copy['preprocessed_' + tweet_col].apply(lambda row: ' '.join(\n",
        "          [word for word in row.split() if (word not in extended_stop_words) and (not re.match(url_re, word)) and (word.find('snowmageddon2020') == -1)]))\n",
        "\n",
        "      # tokenize the tweets\n",
        "      tokenizer = RegexpTokenizer(r'[a-zA-Z]\\w+\\'?\\w*')\n",
        "      df_copy['tokenized_' + tweet_col] = df_copy['preprocessed_' + tweet_col].apply(lambda row: tokenizer.tokenize(row))\n",
        "\n",
        "      if include_bigram:\n",
        "          # Build the bigram and trigram models\n",
        "          texts = df_copy['tokenized_' + tweet_col].values.tolist()\n",
        "          bigram = Phrases(texts, min_count=5, threshold=100)  # higher threshold fewer phrases.\n",
        "          bigram_mod = Phraser(bigram)\n",
        "          df_copy['tokenized_' + tweet_col] = pd.Series([bigram_mod[doc] for doc in texts])\n",
        "\n",
        "      # remove terms with frequency less than 10\n",
        "      if vocab_threshold > 0:\n",
        "          texts = [word for tweet in df_copy.tokenized_text for word in tweet]\n",
        "          vocab = get_frequent_vocab(texts, threshold = vocab_threshold)\n",
        "          df_copy['tokenized_' + tweet_col] = df_copy['tokenized_' + tweet_col].apply(lambda row: [word for word in row if word in vocab])\n",
        "\n",
        "      #remove tweets with length less than two\n",
        "      df_copy = df_copy[df_copy['tokenized_' + tweet_col].map(len) >= 2]\n",
        "\n",
        "      return df_copy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovSxYTYQv-K6"
      },
      "source": [
        "### LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CUWMi07vYtg"
      },
      "source": [
        "#### Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxNGk57CwAqR"
      },
      "source": [
        "if trainLDA:\n",
        "  def build_lda_model(data, num_topics, include_vis = False):\n",
        "      '''\n",
        "      Trains LDA model using Gensim based on parameters\n",
        "      and saves a pyLDAvis visualization of the topics.\n",
        "\n",
        "      :param data: list of tweets to be processed\n",
        "      :param num_topics: number of topics for the model\n",
        "      :param include_vis: flag to include pyLDAvis\n",
        "      :return: LDA model as given by Gensim\n",
        "      '''\n",
        "      # Create Dictionary\n",
        "      id2word = corpora.Dictionary(data)\n",
        "\n",
        "      # Create Corpus: Term Document Frequency\n",
        "      corpus = [id2word.doc2bow(text) for text in data]\n",
        "\n",
        "      # Build LDA model\n",
        "      lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                                  id2word=id2word,\n",
        "                                                  num_topics=num_topics,\n",
        "                                                  random_state=100,\n",
        "                                                  update_every=1,\n",
        "                                                  chunksize=10,\n",
        "                                                  passes=10,\n",
        "                                                  alpha='symmetric',\n",
        "                                                  iterations=100,\n",
        "                                                  per_word_topics=True)\n",
        "\n",
        "      #pyLDAvis\n",
        "      if include_vis:\n",
        "          p = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
        "          pyLDAvis.save_html(p, 'lda_{}.html'.format(num_topics))\n",
        "\n",
        "      return lda_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6i9vfOWvjY5"
      },
      "source": [
        "#### Get Model Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIesbd0dvrUA"
      },
      "source": [
        "if trainLDA:\n",
        "\n",
        "  def top_vocab_lda (lda_model, num):\n",
        "      '''\n",
        "      Gets top vocabulary words for a given LDA model and saves to csv.\n",
        "\n",
        "      :param lda_model: LDA model to be evaluated\n",
        "      :param num: number of vocabulary words assigned to each topic\n",
        "      :return: csv of top words per topic\n",
        "      '''\n",
        "      top_words_per_topic = []\n",
        "      for t in range(lda_model.num_topics):\n",
        "          top_words_per_topic.extend([(t,) + x for x in lda_model.show_topic(t, topn=num)])\n",
        "      return pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'P'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wytnwTI0wBGJ"
      },
      "source": [
        "### BTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8i6-hW8v_oG"
      },
      "source": [
        "#### Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz4SHEhKv8uK"
      },
      "source": [
        "if trainBTM:\n",
        "\n",
        "  def identity_tokenizer (tokens):\n",
        "      return tokens\n",
        "\n",
        "  def build_btm_model(data, num_topics, include_vis= False):\n",
        "      '''\n",
        "      Builds biterm model based on parameters,\n",
        "      saves output model in a npy model\n",
        "      and outputs pyLDAvis model\n",
        "\n",
        "      :param data: list of preprocessed text\n",
        "      :param num_topics: number of topics for biterm model\n",
        "      :param include_vis: flag to include\n",
        "      :return: biterm model and res summary of model\n",
        "      '''\n",
        "\n",
        "      texts = [' '.join(tweet) for tweet in data]\n",
        "\n",
        "      #vectorize tokens\n",
        "      vec = CountVectorizer(stop_words='english', lowercase=False)\n",
        "      X = vec.fit_transform(texts).toarray()\n",
        "\n",
        "      #build vocabulary\n",
        "      vocab = np.array(vec.get_feature_names())\n",
        "      biterms = vec_to_biterms(X)\n",
        "\n",
        "      btm = oBTM(num_topics=num_topics, V=vocab)\n",
        "      biterm_model = btm.fit_transform(biterms, iterations=100)\n",
        "\n",
        "      for i in range(len(texts[:50])):\n",
        "          print(\"{} (topic: {})\".format(texts[i], biterm_model[i].argmax()))\n",
        "\n",
        "      res = topic_summuary(btm.phi_wz.T, X, vocab, 10)\n",
        "      #np.save('btm_summary_{}.npy'.format(num_topics), res)\n",
        "\n",
        "      #pyLDAvis\n",
        "      if include_vis:\n",
        "          vis = pyLDAvis.prepare(btm.phi_wz.T, biterm_model, np.count_nonzero(X, axis=1), vocab, np.sum(X, axis=0))\n",
        "          pyLDAvis.save_html(p, 'biterm_{}.html'.format(num_topics))\n",
        "\n",
        "      return btm, biterm_model, res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31Ul8Hm2wBcx"
      },
      "source": [
        "#### Get Model Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shyp41qVwdN0"
      },
      "source": [
        "if trainBTM:\n",
        "\n",
        "  def top_vocab_btm (res):\n",
        "      '''\n",
        "      Saves top 10 words per topic given model .npy file\n",
        "\n",
        "      :param res: array of model information\n",
        "      :return: saves csv of top words per topic\n",
        "      '''\n",
        "      df = pd.DataFrame()\n",
        "\n",
        "      #save top words per topic\n",
        "      for i in range(len(res.item()['top_words'])):\n",
        "          df['topic_{}'.format(i)] = pd.Series(res.item()['top_words'][i])\n",
        "      #df.to_csv('btm_top_words_{}.csv'.format(len(res.item()['top_words'])))\n",
        "      return df.T.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-utAmWk60aRY"
      },
      "source": [
        "### Retrieving all baselines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5BcoLhI0gKT"
      },
      "source": [
        "if (trainLDA or trainBTM):\n",
        "  # constants and preprocessing\n",
        "  HASHTAGS = {r'#nlwhiteout', r'#nlweather', r'#newfoundland', r'#nlblizzard2020', r'#nlstm2020', r'#snowmaggedon2020', r'#stmageddon2020', r'#stormageddon2020', r'#newfoundland',\n",
        "                              r'#snowpocalypse2020', r'#snowmageddon', r'#nlstm', r'#nlwx', r'#nlblizzard', r'#nlwx', 'snowmaggedon2020', 'newfoundland', r'#nlstorm2020', 'snow', 'st'}\n",
        "\n",
        "  num_topics = [5, 9, 10, 15, 20]\n",
        "\n",
        "  winter_tweets_cleaned = preprocess(df=nlwx_df, extra_stopwords = HASHTAGS)\n",
        "  winter_tweets_cleaned.reset_index(inplace=True, drop=True)\n",
        "  ready_data = winter_tweets_cleaned['tokenized_text'].values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkwoTug4fBQo"
      },
      "source": [
        "#### Example Training Script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzMHSsN1gAfC"
      },
      "source": [
        "num = 10\n",
        "\n",
        "if trainLDA:\n",
        "  lda_model = build_lda_model(ready_data, num_topics = num, include_vis = False)\n",
        "  btm, biterm_model, res = build_btm_model(ready_data, num_topics=num)\n",
        "\n",
        "  #retrieve lda top words for each topic\n",
        "  lda_top_words = pd.DataFrame()\n",
        "  for i in range(lda_model.num_topics):\n",
        "    lda_top_words['topic_{}'.format(i)] = pd.Series([i[0] for i in lda_model.show_topic(i, topn=10)])\n",
        "\n",
        "if trainBTM:\n",
        "  #retrieve top btm words per topic\n",
        "  btm_top_words = pd.DataFrame()\n",
        "  for i in range(len(res.item()['top_words'])):\n",
        "    btm_top_words['topic_{}'.format(i)] = pd.Series(res.item()['top_words'][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6keGf5FCfkY"
      },
      "source": [
        "if trainLDA:\n",
        "  #viewing top words\n",
        "  lda_top_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJUzPLzDe5GW"
      },
      "source": [
        "#### Retrieving Baseline Coherence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5Wf3bZ6e1xL"
      },
      "source": [
        "if trainLDA or trainBTM:\n",
        "  #df to store coherence scores\n",
        "  baseline_coh = pd.DataFrame(columns=['model', 'u_mass', 'c_v', 'c_uci', 'c_npmi'])\n",
        "\n",
        "  # Create Corpus: Term Document Frequency\n",
        "  id2word = corpora.Dictionary(ready_data)\n",
        "  corpus = [id2word.doc2bow(text) for text in ready_data]\n",
        "\n",
        "  #counter for formatting df\n",
        "  i=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMviwJlj0qfj"
      },
      "source": [
        "if trainLDA or trainBTM:\n",
        "  #trains both lda and biterm models\n",
        "  for val in num_topics:\n",
        "          print(\"Building LDA model with {} topics...\".format(val))\n",
        "\n",
        "          if trainLDA:\n",
        "            #coherence model for LDA\n",
        "            lda_model = build_lda_model(ready_data, num_topics = val)\n",
        "            u_mass = CoherenceModel(model=lda_model, corpus=corpus, dictionary=id2word, coherence='u_mass').get_coherence()\n",
        "            c_v = CoherenceModel(model=lda_model, texts=ready_data, dictionary=id2word, coherence='c_v').get_coherence()\n",
        "            c_uci = CoherenceModel(model=lda_model, texts=ready_data, dictionary=id2word, coherence='c_uci').get_coherence()\n",
        "            c_npmi = CoherenceModel(model=lda_model, texts=ready_data, dictionary=id2word, coherence='c_npmi').get_coherence()\n",
        "            lda_coh = [u_mass, c_v, c_uci, c_npmi]\n",
        "            baseline_coh.loc[i] = ['lda_{}'.format(val)] + lda_coh\n",
        "            i += 1\n",
        "          \n",
        "          if trainBTM:\n",
        "            #coherence model for BTM\n",
        "            btm, biterm_model, res = build_btm_model(ready_data, num_topics=val)\n",
        "            ##save top words\n",
        "            top_words_btm = top_vocab_btm(res)\n",
        "            u_mass_b = CoherenceModel(topics=top_words_btm, corpus=corpus, dictionary=id2word, coherence='u_mass').get_coherence()\n",
        "            c_v_b = CoherenceModel(topics=top_words_btm, texts=ready_data, dictionary=id2word, coherence='c_v').get_coherence()\n",
        "            c_uci_b = CoherenceModel(topics=top_words_btm, texts=ready_data, dictionary=id2word, coherence='c_uci').get_coherence()\n",
        "            c_npmi_b = CoherenceModel(topics=top_words_btm, texts=ready_data, dictionary=id2word, coherence='c_npmi').get_coherence()\n",
        "            btm_coh = [u_mass_b, c_v_b, c_uci_b, c_npmi_b]\n",
        "            baseline_coh.loc[i] = ['btm_{}'.format(val)] + btm_coh\n",
        "            i += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPruU6aE2dln"
      },
      "source": [
        "if trainLDA or trainBTM:\n",
        "  baseline_coh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03mlne7pwH1k"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNMst9xmj-Wj"
      },
      "source": [
        "### Coherence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjE3zFvzoGpM"
      },
      "source": [
        "def return_coherence(features, topics, coherence_type):\n",
        "    dct = Dictionary(features)\n",
        "    topics = topics.T.values.tolist()\n",
        "    if coherence_type == 'u_mass':\n",
        "        try:\n",
        "            bow_corpus = [dct.doc2bow(f) for f in features]\n",
        "            cm = CoherenceModel(topics=topics, corpus=bow_corpus, dictionary=dct, coherence=coherence_type)\n",
        "            coherence = cm.get_coherence()  # get coherence value\n",
        "        except:\n",
        "            coherence = 0\n",
        "\n",
        "    elif coherence_type in ['c_v', 'c_uci', 'c_npmi']:\n",
        "        try:\n",
        "            cm = CoherenceModel(topics=topics, texts=features, dictionary=dct, coherence=coherence_type)\n",
        "            coherence = cm.get_coherence()  # get coherence value\n",
        "        except:\n",
        "            coherence = 0\n",
        "\n",
        "    else:\n",
        "        print(f\"'{coherence_type}' is not a coherence model.\" \n",
        "              \"Use one of the following arguments:'u_mass', 'c_v', 'c_uci', 'c_npmi'.\")\n",
        "    \n",
        "    return coherence\n",
        "\n",
        "def output_coherence(n_topics, h, n, s, m, p, comp, \n",
        "                     features, topics, embed_name, coh_type, topic_model = \"kmeans\"):\n",
        "    topic_paths = []\n",
        "    df = {'embeddings': [], 'model': [], 'components':[], 'topics': [], \n",
        "          'ngrams_per_topic':[],'ct': [], 'coherence': [], 'hashtags': [],\n",
        "          'phrasing':[],'max_df':[],'stf':[],'ngrams':[]}\n",
        "    coherence = return_coherence(features, topics, coherence_type = coh_type)\n",
        "    print(f\"Getting {coh_type} coherence for {embed_name} with {n_topics} topics and {comp} components.\")\n",
        "    df['embeddings'].append(embed_name)\n",
        "    df['model'].append(topic_model)\n",
        "    df['components'].append(comp)\n",
        "    df['topics'].append(n_topics)\n",
        "    df['ngrams_per_topic'].append(10)\n",
        "    df['ct'].append(coh_type)\n",
        "    df['coherence'].append(coherence)\n",
        "    df['hashtags'].append(h)\n",
        "    df['phrasing'].append(p)\n",
        "    df['max_df'].append(m)\n",
        "    df['stf'].append(s)\n",
        "    df['ngrams'].append(n)\n",
        "    df = pd.DataFrame.from_dict(df)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st1ArWuhkH1_"
      },
      "source": [
        "def get_coherence(ts, n_topics, hashtag_opts, ngrams, stf, max_df, phrasing, components, ct, embed_name):\n",
        "  c_dfs = []\n",
        "  for t in n_topics:\n",
        "    for h in hashtag_opts:\n",
        "      for n in ngrams:\n",
        "        for s in stf:\n",
        "          for m in max_df:\n",
        "            for p in phrasing:\n",
        "              for comp in components:\n",
        "                for c in ct:\n",
        "                  df = output_coherence(t, h, n, s, m, p, comp,\n",
        "                                        ts.hyperparams[embed_name][t]['hashtags'][h]['ngram'][n]['stf'][s]['max_df'][m]['phrasing'][p]['features'],\n",
        "                                        ts.hyperparams[embed_name][t]['hashtags'][h]['ngram'][n]['stf'][s]['max_df'][m]['phrasing'][p][f'topics_{comp}'],\n",
        "                                        embed_name = embed_name,\n",
        "                                        coh_type = c)\n",
        "                  c_dfs.append(df)\n",
        "  coherence = pd.concat(c_dfs).groupby('ct', group_keys = False).apply(\n",
        "        pd.DataFrame.sort_values, 'coherence', ascending=False).reset_index(drop = 'True')\n",
        "  return coherence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCik0Lj_nWzm"
      },
      "source": [
        "ct = ['c_v','c_npmi']\n",
        "components = ['attn', 'tfidf', 'tfidf_attn']\n",
        "\n",
        "coherence_fte = get_coherence(ts, \n",
        "                              n_topics, \n",
        "                              hashtag_opts, \n",
        "                              ngrams, \n",
        "                              stf, \n",
        "                              max_df, \n",
        "                              phrasing, \n",
        "                              components, \n",
        "                              ct,\n",
        "                              'fte')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "objzVK-3ndcm"
      },
      "source": [
        "coherence_bert = get_coherence(ts_bert, \n",
        "                               n_topics, \n",
        "                               hashtag_opts, \n",
        "                               ngrams, \n",
        "                               stf, \n",
        "                               max_df, \n",
        "                               phrasing, \n",
        "                               components, \n",
        "                               ct, \n",
        "                               'bert')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqd7w9dt0zeh"
      },
      "source": [
        "def best_model(df, ct = 'c_v'):\n",
        "    if ct == 'c_v':\n",
        "        df = df[df['ct'] == 'c_v']\n",
        "        max_df = df.loc[df.groupby(['topics','embeddings','components'])['coherence'].idxmax()]\n",
        "    if ct == 'u_mass':\n",
        "        df = df[df['ct'] == 'u_mass']\n",
        "        max_df = df.loc[df.groupby(['topics','embeddings','components'])['coherence'].idxmin()]\n",
        "    if ct == 'c_npmi' or df.ct.all() == 'c_uci':\n",
        "        df = df[df['ct'] == 'c_npmi']\n",
        "        max_df = df.loc[df.groupby(['topics','embeddings','components'])['coherence'].idxmin()]\n",
        "    return max_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R_rzPBXoCOO"
      },
      "source": [
        "best_fte_cv = best_model(coherence_fte, 'c_v')\n",
        "best_fte_npmi = best_model(coherence_fte, 'c_npmi')\n",
        "best_fte = pd.concat([best_fte_cv, best_fte_npmi])\n",
        "\n",
        "best_bert_cv = best_model(coherence_bert, 'c_v')\n",
        "best_bert_npmi = best_model(coherence_bert, 'c_npmi')\n",
        "best_bert = pd.concat([best_bert_cv, best_bert_npmi])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF7GaWyBwrNR"
      },
      "source": [
        "### Keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-ozXVzTDBmn"
      },
      "source": [
        "best_cv_grouped = best_fte_cv[best_fte_cv.topics == 9].sort_values('coherence',ascending = False).groupby('components').head(1)\n",
        "best_a = best_cv_grouped[best_cv_grouped.components == 'attn'].to_dict(orient=\"records\")[0]\n",
        "best_t = best_cv_grouped[best_cv_grouped.components == 'tfidf'].to_dict(orient=\"records\")[0]\n",
        "best_ta = best_cv_grouped[best_cv_grouped.components == 'tfidf_attn'].to_dict(orient=\"records\")[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX7IVJ5THCLw"
      },
      "source": [
        "print(ts.hyperparams['fte'][best_a['topics']]['hashtags'][best_a['hashtags']]['ngram'][best_a['ngrams']]['stf'][best_a['stf']]['max_df'][best_a['max_df']]['phrasing'][best_a['phrasing']]['topics_attn'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNS074lKChW0"
      },
      "source": [
        "print(ts.hyperparams['fte'][best_t['topics']]['hashtags'][best_t['hashtags']]['ngram'][best_t['ngrams']]['stf'][best_t['stf']]['max_df'][best_t['max_df']]['phrasing'][best_t['phrasing']]['topics_attn'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-V3jhV6KDyN"
      },
      "source": [
        "print(ts.hyperparams['fte'][best_ta['topics']]['hashtags'][best_ta['hashtags']]['ngram'][best_ta['ngrams']]['stf'][best_ta['stf']]['max_df'][best_ta['max_df']]['phrasing'][best_ta['phrasing']]['topics_attn'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1ZFj8pTwdcX"
      },
      "source": [
        "### Automatic evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9PgDWEn1lWa"
      },
      "source": [
        "def concat_dfs(fte_df, lda_df, save = True):\n",
        "    output = fte_df[fte_df.embeddings.isin(['fte','bert'])]\n",
        "    output = output.replace(\n",
        "        {'tfidf_attn':'tfidf-attn',\n",
        "         'fte': 'FTE',\n",
        "         'bert': 'BERT'})\n",
        "    output = output.rename({'embeddings': 'Model', \n",
        "                            'components': 'Components'}, axis='columns')\n",
        "    output_cv = output[output['ct'].str.contains('c_v')]\n",
        "    output_npmi = output[output['ct'].str.contains('c_npmi')]\n",
        "    output_cv = output_cv.rename({'coherence': 'Coherence_CV'}, axis='columns')\n",
        "    output_cv['Coherence_NPMI'] = output_npmi['coherence'].tolist()\n",
        "    output_plot = output_cv\n",
        "    output_full = output_cv[['Components','Model','topics',\n",
        "                             'Coherence_CV','Coherence_NPMI']].append(lda_df, ignore_index=True)\n",
        "    output_full['Coherence_NPMI_abs'] = output_full.Coherence_NPMI.abs()\n",
        "    output_full['Model, components'] = output_full[\"Model\"] + \", \" + output_full[\"Components\"]\n",
        "    output_full['Model, components'] = np.where(output_full['Model, components'].isin([np.nan]), \n",
        "                                                output_full['Model'], \n",
        "                                                output_full['Model, components'])\n",
        "    return(output_full)\n",
        "\n",
        "def output_plot(data, y, ylab, filename):\n",
        "    sns.set(style=\"ticks\", font_scale=1.2)\n",
        "    fig, ax = plt.subplots()\n",
        "    p = sns.lineplot(data= data, \n",
        "                     x='topics', \n",
        "                     y='Coherence_CV',\n",
        "                     hue = 'Model, components',\n",
        "                     style = 'Model, components',\n",
        "                     palette='rocket',\n",
        "                     markers=True,\n",
        "                     ax=ax)\n",
        "    plt.xlabel('Number of topics')\n",
        "    plt.ylabel(ylab)\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    ax.legend(handles=handles, labels=labels)\n",
        "    p.set_xticks(range(5,16))\n",
        "    p.set_xticklabels(range(5,16))\n",
        "    p.figure.savefig(f\"{filename}.png\",dpi=300, bbox_inches=\"tight\")\n",
        "    plt.clf()\n",
        "\n",
        "def filter_df(df, *args):\n",
        "    for k, v in args:\n",
        "        df = df[df[k].isin(v)]\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzSkfz0k1gaC"
      },
      "source": [
        "bert_models = pd.concat([best_fte, best_bert])\n",
        "all_models = concat_dfs(bert_models, lda_df, save = True)\n",
        "output_plot(data = filter_df(all_models, \n",
        "                              ('Model', ['FTE','BERT','BTM','LDA']),\n",
        "                              ('Components', ['tfidf-attn', np.nan])), \n",
        "            y = 'Coherence_CV', \n",
        "            ylab = 'Coherence ($\\mathregular{C_{v}}$)',\n",
        "            filename = 'CV_plot_best_FTA_vs_baselines')\n",
        "\n",
        "output_plot(data = filter_df(all_models, \n",
        "                             ('Model', ['FTE']),\n",
        "                             ('Components', ['tfidf-attn', 'tfidf','attn'])), \n",
        "            y = 'Coherence_CV', \n",
        "            ylab = 'Coherence ($\\mathregular{C_{v}}$)', \n",
        "            filename = 'CV_plot_FTA')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWmPXBbX3Att"
      },
      "source": [
        "Image('CV_plot_best_FTA_vs_baselines.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kya8U2un5-Ne"
      },
      "source": [
        "Image('CV_plot_FTA.png')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}